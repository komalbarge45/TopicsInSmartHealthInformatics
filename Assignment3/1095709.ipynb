{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics in Smart Health Informatics\n",
    "### Assignment 3\n",
    "\n",
    "Name: Komal Barge (1095709)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given dataset of 1500 CT scans of various parts of the body such as abdomen, chest and head has to be trained with machine leanring models to classify the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the current directory to given path\n",
    "os.chdir(\"./Documents/DS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the images from set path\n",
    "filenames = os.listdir()\n",
    "categories = []\n",
    "X = []\n",
    "\n",
    "#fetch the target labels for all the images\n",
    "for filename in filenames:\n",
    "    category = filename.split('_')[0]\n",
    "    if category == 'Abdomen':\n",
    "        categories.append(0)\n",
    "    elif category == 'Chest':\n",
    "        categories.append(1)\n",
    "    else:\n",
    "        categories.append(2)\n",
    "    img = Image.open(filename)        #open an image\n",
    "    img = img.resize((32,32))         #resize an image to 32x32 size\n",
    "    imgArray = np.asarray(img)\n",
    "    X.append(imgArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the image to send it in convolutional neural networks.\n",
    "X = np.array(X).astype(np.float32)\n",
    "X = np.reshape(X, (-1,32,32,1))\n",
    "y = np.array(categories).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data in 70:30 ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the target labels to categorical matrix\n",
    "from keras.utils import to_categorical\n",
    "yTrain = to_categorical(y_train, 3)\n",
    "yTest = to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############Preprocess an image with imageDataGenerator#################\n",
    "\n",
    "#Image Augmentations techniques are methods of artificially increasing the variations of images \n",
    "#in our data-set by using horizontal flips, rotations, changing the zoom and shear range etc.\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "train_datagen.fit(X_train)\n",
    "train_generator = train_datagen.flow(X_train, yTrain, batch_size=32)        #to generate batches of augmented train images\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) \n",
    "test_datagen.fit(X_test)\n",
    "test_generator = test_datagen.flow(X_test)                              #to generate batches of augmented test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a CNN model to extract features from the images.\n",
    "activationFunction = 'relu'\n",
    "def getCNNModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(25, kernel_size=(3,3) ,input_shape=X[0].shape,    \n",
    "                  activation=activationFunction))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size=(2,2), activation=activationFunction))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv2D(8, kernel_size=(2,2), activation=activationFunction))\n",
    "    model.add(MaxPooling2D(pool_size=1))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation=activationFunction, name = \"extractedFeature\"))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "modelCNN = getCNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "33/33 [==============================] - 2s 66ms/step - loss: 0.4665 - accuracy: 0.8000\n",
      "Epoch 2/5\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0530 - accuracy: 0.9838\n",
      "Epoch 3/5\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0163 - accuracy: 0.9990\n",
      "Epoch 4/5\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0201 - accuracy: 0.9962 0s - loss: 0.0225 - accura\n",
      "Epoch 5/5\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0180 - accuracy: 0.9924\n"
     ]
    }
   ],
   "source": [
    "#we will fit the model augmented images to get the trained layers with proper weight adjustments.\n",
    "history = modelCNN.fit_generator(train_generator,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_input (InputLayer)    [(None, 32, 32, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 30, 30, 25)        250       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 30, 25)        100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 16)        1616      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 6, 8)           520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "extractedFeature (Dense)     (None, 50)                14450     \n",
      "=================================================================\n",
      "Total params: 16,936\n",
      "Trainable params: 16,886\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Extract features from second last dense layer in above CNN architecture\n",
    "extractedModel = keras.models.Model(inputs=modelCNN.input, outputs=modelCNN.get_layer('extractedFeature').output)\n",
    "extractedModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features for original training images from trained extracted CNN model\n",
    "train_features = extractedModel.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features for original testing images from trained extracted CNN model\n",
    "test_features = extractedModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for showing classification results\n",
    "def showClassficationResults(true_val, predicted_val):\n",
    "    print(\"Detailed Classification report: \\n\",classification_report(true_val, predicted_val))\n",
    "    print('\\n')\n",
    "    print(\"Confusion matrix: \\n\",confusion_matrix(true_val, predicted_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN classifier on extracted feature images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "bestAccuracy = 0\n",
    "#K neighbour values to use in KNN classifier\n",
    "K = [3,5,7,9]\n",
    "\n",
    "for i in K:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(train_features, y_train)\n",
    "    pred_knn = knn.predict(test_features)                   #Predict test features of images\n",
    "    modelAccuracy = accuracy_score(y_test, pred_knn)          #Performace metric for multiclass labels\n",
    "    accuracy.append(modelAccuracy)\n",
    "    if bestAccuracy < modelAccuracy:\n",
    "        bestAccuracy = modelAccuracy                        #Assign best accuracy for optimal k value\n",
    "        bestModel = knn                                     #get the best model from optimal k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#A graph of K values and their corresponding accuracy performance\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(K,accuracy,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Accuracy vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       148\n",
      "         1.0       1.00      1.00      1.00       141\n",
      "         2.0       1.00      1.00      1.00       161\n",
      "\n",
      "    accuracy                           1.00       450\n",
      "   macro avg       1.00      1.00      1.00       450\n",
      "weighted avg       1.00      1.00      1.00       450\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      " [[148   0   0]\n",
      " [  0 141   0]\n",
      " [  0   0 161]]\n"
     ]
    }
   ],
   "source": [
    "#Save a model and load the same to get the testing accuracy with the optimal model\n",
    "pickle.dump(bestModel, open('../1095709_KNN.pkl','wb'))\n",
    "loadmodel_knn = pickle.load(open('../1095709_KNN.pkl','rb'))\n",
    "loaded_knn_pred = loadmodel_knn.predict(test_features)\n",
    "showClassficationResults(y_test,loaded_knn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optimal parameters used for KNN\n",
    "loadmodel_knn.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest on extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    3.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_score=False,\n",
       "                                                    random_state=None,\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   iid='warn', n_iter=15, n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'n_estimators': [20, 30, 40, 50, 60, 70,\n",
       "                                                         80, 90]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#############Hyperparameters to be tuned using random search###############\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in range(20,100,10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 15, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       148\n",
      "         1.0       1.00      1.00      1.00       141\n",
      "         2.0       1.00      1.00      1.00       161\n",
      "\n",
      "    accuracy                           1.00       450\n",
      "   macro avg       1.00      1.00      1.00       450\n",
      "weighted avg       1.00      1.00      1.00       450\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      " [[148   0   0]\n",
      " [  0 141   0]\n",
      " [  0   0 161]]\n"
     ]
    }
   ],
   "source": [
    "#Save the model and load it to test images. Show the confusion matrix for the optimal performance.\n",
    "pickle.dump(rf_random, open('../1095709_RF.pkl','wb'))\n",
    "loadmodel_rfc = pickle.load(open('../1095709_RF.pkl','rb'))\n",
    "loaded_rfc_pred = loadmodel_rfc.predict(test_features)\n",
    "showClassficationResults(y_test,loaded_rfc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 80, 'max_features': 'sqrt', 'max_depth': 20}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best hyperparameters used for RF prediction\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
